{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Enron data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poi_id\n",
    "data_dict = poi_id.prepare_data(None, load=True)\n",
    "labels = []\n",
    "for key, value in data_dict.items():\n",
    "    labels.append(value[\"poi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'sub_to_below is a confirm list of attende for tomorrow meet in 40c2 for those of you connect by confer call the number 8009919019 passcod 6017616 look forward to see everyon at 900 am tomorrow stacey', u'sub_to_dick this is the present steve would be use on monday i call you to coordin but there was no answer at your offic could you pleas confirm me that you receiv this messag and that the present would be up and run on monday steve will have a backup with him just in case pleas do not print forward or distribut copi of this present sinc it is against our compani polici thank in advanc best regard ursula brenner enron corp 17133453787 this messag includ ani attach contain confidenti inform intend for a specif individu and purpos and is protect by law if you are not the intend recipi you should delet this messag and are herebi notifi that ani disclosur copi or distribut of this messag or the take of ani action base on it is strict prohibit', u'word_intermin', u'sub_to_rw pleas respond to the messag below thank you jim', u'word_oftentim', u'word_obsess', u'sub_from_you probabl saw the announc that mike mcconnel is now ceo of enron global market egm will be focuss on a number of new and exist market includ coal lng crude and product interest rate agricultur commod equiti weather etc one of the market mike group is activ consid is transport capac i mention your work in the area he is anxious to see it and meet with you after he has had a chanc to review it', u'sub_to_fyi jeff donohu is ok with the abb exclus agreement i was serious about parquet and mcdonald chris calger 5034643735', u'sub_to_mark per your request it look like your guess was right on target i calcul that 7062 of the 2001 ewm legal plan for insid cost onli is for ena busi unit attach for your refer is my excel worksheet along with a page explain my calcul pleas call me if you have ani question or need further inform deb deb korkma enron north america corp ena legal dept 1400 smith street room 3819 houston tx 77002 phone 7138535448 fax 7136463393', u'sub_to_i ad senat kinder telephon number to the last paragraph', u'sub_to_it may be of interest to you lm', u'sub_to_john now that ive heal from the 175 mile bike experi i want to thank you for your generous donat put toward the sponsorship of my ms150 houstonaustin bicycl ride it was much appreci and the money all goe to a good and worthi caus contrari to my initi gut impuls the money did not go to the scott tholan new bike fund serious though thank for your support next year you ride too scott ps i recent sent in your check and it should be deposit by the ms organ in the next sever day', u'sub_to_ill tri but no promis if i cant is there someon els from the environment area that would be good like mike terraso or terenc thorn interest i have them both list as chief environment offic it would be better if the person was an offic anyway if i can swing the public affair can we make it mark palmer sinc hes been veri involv with us and is a vp lynn shapiro richard richardshapiroenroncom 101901 0955am not in theorybut and this is high confidenti jeff is likely999 to be a victim of downsiz cant we have public affair back', u'word_nisho', u'sub_from_darrel thank a lot a quick question we havent receiv your invoic for the last few model paper review pleas make sure that we are bill clewlowstrickland book is out i shall send you a copi today vinc', u'sub_to_i know it crazi around here but if you have time to review the attach document theyr short i would appreci your comment especi sinc youv wit a focus group befor they are the messag and focus group guid for tuesdaywednesday ny research ill be here all day monday if you want to get me comment by then thank you nyectestmessagesdoc focusgroupguidedoc', u'sub_to_ryan in the follow up to the meet we had on wednesday i would like to reinforc one point i made i feel strong that i cannot support the valuat my group has produc so far for the ljmraptor relat transact without examin of all the relat legal document i feel that we did solid work base on verbal inform but i cannot guarante the qualiti of the final product without look at the contract these transact are too complex and controversi to bypass due dilig requir that you would expect from ani profession vinc kaminski', u'sub_to_i onli need a ride home from the board meet becaus ill have to ride out there with veronica earli to set up im glad you like the note i love you and will talk to you tonight love caron', u'word_sentenceperhap', u'sub_from_andi i show he is to attend pleas let me know if there is anyon further that you would like to attend k', u'sub_from_what is this about', u'word_alexmckintoshbnpparibascom', u'sub_to_mark taylor vice presid and general counsel enron wholesal servic 1400 smith street eb3892 houston texa 77008 7138537459 7136463490 fax', u'word_competitorsit', u'sub_from_mike we can get you this info on tuesday stu pleas provid intl stuff thank georg', u'sub_to_pleas note the follow addit to the credit reserv for octob 2001 cp alberta direct hedg deal no 812159 deal date 100901 deal specif ecc sell financi swap to cp term 112002 thru 12312007 volum 317494 mwh 7x24 price fix 67044 cad region alberta power pool tradermidmarket john zufferliderek davi credit reserv amount 67000 cad rac transact confirm under 9401 isda master agreement between ecc and enron direct canada corp pleas call with question or comment thank you wendi conwel', u'sub_to_john coupl comment transmiss feder and state jurisdict i did not fulli understand state have no jurisdict over transmiss facil and this provis prevent larg industri custom from tie direct into the transmiss grid and avoid state charg there is bypass to the interst pipelin now it seem that industri should be abl to connect to the interst grid especi when they have onsit generat that they want to sell into the grid market power market monitor in the se rto mediat i rais this possibl about ferc have region offic and be the market monitor instead of establish these elabor mm structur everyon thought that this is a day 5 or 6 possibl and mani of the muni and coop realli want the elabor structur separ mm compani etc at least initi electr suppli interconnect standard pay direct cost direct cost should not includ network upgrad that benefit other custom of the grid ferc recent issu an order in a consum case that said interconnect custom do not have to pay the network upgrad they are roll into rate this is someth enron encount on most everi interconnect we attempt the util typic put in all kind of cost and tri to direct bill us feder electr utili tva power sale limit tva wholesal power sale outsid the region to sale of excess power under the current schedul rule day ahead balanc schedul requir tva could have alot of latitud and discret in what it call excess if we get the congest manag we want this would be less of a problem regul of tva tran system tva has argu feder law prevent i thought tva was say it is difficult for tva to join an rto not imposs under current law they rais the letter that bpa appar receiv encourag it to join an rto real time price is good thank', u'sub_to_wait for the number and rational i will get back to you short regard delainey', u'word_bpamoco', u'sub_to_here a good one peabodi coal bb rate cut us off we ask their trader whi and he had no idea']\n"
     ]
    }
   ],
   "source": [
    "print data_dict[data_dict.keys()[0]].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Email Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'word_unttil', u'word_intermin', u'word_oftentim', u'word_obsess', u'word_nisho', u'word_sentenceperhap', u'word_alexmckintoshbnpparibascom', u'word_competitorsit', u'word_paulin', u'word_issler']\n",
      "['sub_from_Opportunities at ENA', 'sub_from_Received fax from 01372 386764', 'sub_from_EES Associate and Analyst Mid-year 2001 PRC - SAVE THE DATE', 'sub_from_Credit Watch List--3/20/01', 'sub_to_Tom White Expenses', 'sub_from_California Lawmakers Vote to Limit Power Costs - WSJ', 'sub_from_Enron Europe', 'sub_to_Last Chance! Tour Championship Tickets!', 'sub_to_Capacity Subscription Strategy', 'sub_to_ASE Hypertiles from Final Report Out']\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "labels = []\n",
    "names = []\n",
    "for key, value in data_dict.items():\n",
    "    labels.append(value[\"poi\"])\n",
    "    #value.pop(\"poi\",None)\n",
    "    features.append(value)\n",
    "    names.append(key)\n",
    "            \n",
    "get_email_text = poi_id.SelectMatchFeatures(feature_match=\"word_.*\")\n",
    "email_text = get_email_text.fit_transform(features)\n",
    "\n",
    "print get_email_text.get_feature_names()[:10]\n",
    "\n",
    "get_sub = poi_id.SelectMatchFeatures(feature_match=\"sub_.*\")\n",
    "subs = get_sub.fit_transform(features)\n",
    "print get_sub.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-866f8e2c10d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdropper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoi_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropSelectedFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_match_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word_.*\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sub_.*\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Moritz\\Anaconda3\\envs\\Udacity_ML_mini 1\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Moritz\\Workspace\\ud120-projects\\final_project\\poi_id.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mnew_item\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_feature_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"NaN\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dropper = poi_id.DropSelectedFeatures(drop_match_keys=[\"word_.*\", \"sub_.*\"])\n",
    "cleaned = dropper.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first,document.',\n",
    "    'This is the second,second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the,first, document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.568946952553\n",
      "  (0, 0)\t0.593800244493\n",
      "  (0, 3)\t0.568946952553\n",
      "  (1, 0)\t0.593800244493\n",
      "  (1, 8)\t0.568946952553\n",
      "  (1, 6)\t0.568946952553\n",
      "  (2, 0)\t0.46263733109\n",
      "  (2, 2)\t0.886547629787\n",
      "  (3, 0)\t0.670543674943\n",
      "  (3, 5)\t0.428318876537\n",
      "  (3, 4)\t0.428318876537\n",
      "  (3, 1)\t0.428318876537\n",
      "[u'', u' document?', u'and the third one.', u'document.', u'first', u'is this the', u'second document.', u'this is the first', u'this is the second']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>document?</th>\n",
       "      <th>and the third one.</th>\n",
       "      <th>document.</th>\n",
       "      <th>first</th>\n",
       "      <th>is this the</th>\n",
       "      <th>second document.</th>\n",
       "      <th>this is the first</th>\n",
       "      <th>this is the second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.593800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568947</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.593800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.462637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.670544</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              document?  and the third one.  document.     first  is this the  \\\n",
       "0  0.593800    0.000000            0.000000   0.568947  0.000000     0.000000   \n",
       "1  0.593800    0.000000            0.000000   0.000000  0.000000     0.000000   \n",
       "2  0.462637    0.000000            0.886548   0.000000  0.000000     0.000000   \n",
       "3  0.670544    0.428319            0.000000   0.000000  0.428319     0.428319   \n",
       "\n",
       "   second document.  this is the first  this is the second  \n",
       "0          0.000000           0.568947            0.000000  \n",
       "1          0.568947           0.000000            0.568947  \n",
       "2          0.000000           0.000000            0.000000  \n",
       "3          0.000000           0.000000            0.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'[^,]*')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print X\n",
    "print vectorizer.get_feature_names()\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "#type(X.toarray())\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Udacity_ML_mini 1]",
   "language": "python",
   "name": "conda-env-Udacity_ML_mini 1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
